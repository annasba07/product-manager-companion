{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Medium articles with tags ML/DL/AI using BS & SELENIUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries\n",
    "import urllib.request\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the links we wanna scrape\n",
    "arificial_intelligence = \"https://medium.com/tag/product-management\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic loading with selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('C:/Software Projects/chromedriver_win32/chromedriver.exe')\n",
    "driver.get(arificial_intelligence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "# Web scrapper for infinite scrolling page \n",
    "time.sleep(2)  # Allow 2 seconds for the web page to open\n",
    "scroll_pause_time = 1 # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
    "screen_height = driver.execute_script(\"return window.screen.height;\")   # get the screen height of the web\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    # scroll one screen height each time\n",
    "    driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "    i += 1\n",
    "    time.sleep(scroll_pause_time)\n",
    "    # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "    scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "    # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "    if (screen_height) * i > scroll_height:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the webpage that is displayed till you reach the end to collect all tagged articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prettify res with beautiful soup\n",
    "\n",
    "soup = BeautifulSoup(res, 'lxml')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install BeautifulSoup4\n",
    "!pip install lxml\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(res, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = soup.find_all('div', {\"class\": \"ke kf kg kh ki l\"})\n",
    "b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text from beautiful soup object and put it in a list\n",
    "\n",
    "\n",
    "\n",
    "text = []\n",
    "for i in b:\n",
    "    text.append(i.text)\n",
    "\n",
    "text[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get href link from bs object and put in list\n",
    "\n",
    "c = soup.find_all('a', {\"class\": \"ae af ag ah ai aj ak al am an ao ap aq ar as\"})\n",
    "\n",
    "\n",
    "links = []\n",
    "for i in c:\n",
    "    links.append(i.get('href'))\n",
    "\n",
    "\n",
    "links\n",
    "\n",
    "# remove all items that include the word signin from links\n",
    "\n",
    "links2 = [i for i in links if 'signin' not in i]\n",
    "#links2 = [i for i in links2 if '?' not in i]\n",
    "\n",
    "# remove all duplicates form links2\n",
    "\n",
    "links3 = list(dict.fromkeys(links2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(links3))\n",
    "print(len(text))\n",
    "\n",
    "df1 = pd.DataFrame({'text': text})\n",
    "\n",
    "df2 = pd.DataFrame({'links': links3})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df1 and df2 to csv\n",
    "\n",
    "df1.to_csv('pmcontentmediumtext.csv', index=False)\n",
    "df2.to_csv('pmcontentmediumlinks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the right div and class for all links\n",
    "extract = soup.find_all('div', {\"class\": \"q ab\"})\n",
    "\n",
    "#finding the right div and class for all text\n",
    "extract2 = soup.find_all('div', {\"class\": \"jx l\"})\n",
    "\n",
    "len(extract)\n",
    "len(extract2)\n",
    "\n",
    "# print length of extract and extract2\n",
    "\n",
    "print(len(extract))\n",
    "print(len(extract2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find h2 child of extract\n",
    "\n",
    "titles = []\n",
    "\n",
    "for i in extract2:\n",
    "    titles.append(i.find('h2').text)\n",
    "\n",
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prettify extract[0]\n",
    "\n",
    "temp = extract[0].find_all('div', {\"class\": \"ab q\"})\n",
    "\n",
    "# find "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find href from temp\n",
    "\n",
    "links2 = []\n",
    "\n",
    "for i in temp:\n",
    "    links2.append(i.find('a').get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "\n",
    "for i in extract:\n",
    "    thing = i.find_all('div', {\"class\": \"ab q\"})\n",
    "    for i in thing:\n",
    "        links.append(i.find('a').get('href'))\n",
    "\n",
    "\n",
    "print(len(links))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add https://www.medium.com to the beginning of each item in the list\n",
    "\n",
    "finallinks = ['https://www.medium.com' + i for i in links]\n",
    "finallinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(finallinks)\n",
    "len(titles)\n",
    "\n",
    "print('there are {} words and {} links'.format(len(titles), len(finallinks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('good work!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'titles': titles, 'links': finallinks})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape all text from each link in links4\n",
    "\n",
    "import urllib.request\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "content = []\n",
    "for i in finallinks:\n",
    "    url = i\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    content.append(soup.get_text())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add text2 to df\n",
    "\n",
    "df['content'] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'][0]\n",
    "\n",
    "# add a column to df which shows the length of each article\n",
    "\n",
    "df['content_length'] = df['content'].str.len()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df as csv\n",
    "\n",
    "df.to_csv('pmmediumdataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "COMPLETIONS_MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = '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'\n",
    "\n",
    "!pip install plotly\n",
    "!pip install -U scikit-learn scipy matplotlib\n",
    "\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from csv and store in df\n",
    "import pandas as pd\n",
    "\n",
    "input_datapath = 'product_management2_with_embeddings.csv'  # to save space, we provide a pre-filtered dataset\n",
    "df2 = pd.read_csv(input_datapath, index_col=0)\n",
    "#df2 = df2[['Text', 'Links', 'Content', 'Content_Length', 'Embeddings']]\n",
    "df2 = df2.dropna()\n",
    "df2['combined'] = \"Title: \" + df2.text.str.strip() + \"; Content: \" + df.content.str.strip()\n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df['combined'] = \"Title: \" + df.titles.str.strip() + \"; Content: \" + df.content.str.strip()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings for each article in df and store in a new column\n",
    "\n",
    "df[\"embeddings\"] = df.combined.apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('pmmediumdataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Split the article into sentences\n",
    "sentences = sent_tokenize(df.combined[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is the future of product management? \\n\"\n",
    "query = \"Q: \" + question + \" A: \"\n",
    "\n",
    "embeddingpm = get_embedding(\n",
    "        question,\n",
    "        engine=\"text-embedding-ada-002\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"similarities\"] = df.embeddings.apply(lambda x: np.dot(np.array(embeddingpm), np.array(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"similaritiescosine\"] = df.embeddings.apply(lambda x: cosine_similarity(x, embeddingpm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df as csv\n",
    "\n",
    "df2.to_csv('product_management2_with_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order df2 by similarity descending\n",
    "\n",
    "df = df.sort_values(by=['similarities'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index for df2\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"contentlength\"] = df2.content.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "context = df.combined[0]\n",
    "#+ df.combined[1] \n",
    "#+ df.combined[2]  + df.combined[3]\n",
    "len(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETIONS_MODEL = \"text-davinci-002\"\n",
    "\n",
    "prompt = \"\"\"You are a product management expert. Answer the question in detail using the provided context, and if the answer is not contained in the text above then answer it how you normally would. Explain things in a lot of detail.   \\n\"\"\" + context + query\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "\n",
    "def search_content(df2, query, n=3, pprint=True):\n",
    "    embedding = get_embedding(\n",
    "        query,\n",
    "        engine=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    df2[\"similarities\"] = df2.ada_search.apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "    res = (\n",
    "        df2.sort_values(\"similarities\", ascending=False)\n",
    "        .head(n)\n",
    "    )\n",
    "   \n",
    "    if pprint:\n",
    "        for r in res:\n",
    "            print(r[:200])\n",
    "            print()\n",
    "    return res\n",
    "\n",
    "\n",
    "res = search_content(df2, \"product\", n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search articles based on query\n",
    "\n",
    "def search_articles(df2, query, n=3, pprint=True):\n",
    "   embedding = get_embeddings(query, engine='text-embedding-ada-002')\n",
    "   df2['similarities'] = df2.embeddings.apply(lambda x: cosine_similarity(x, embedding))\n",
    "   res = df2.sort_values('similarities', ascending=False).head(n)\n",
    "   return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the search\n",
    "\n",
    "result = search_articles(df2, \"what is product management?\", n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing open ai tutorial\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "input_datapath = 'Reviews.csv'  # to save space, we provide a pre-filtered dataset\n",
    "df = pd.read_csv(input_datapath, index_col=0)\n",
    "df = df[['Time', 'ProductId', 'UserId', 'Score', 'Summary', 'Text']]\n",
    "df = df.dropna()\n",
    "df['combined'] = \"Title: \" + df.Summary.str.strip() + \"; Content: \" + df.Text.str.strip()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort df by combinedlength desc\n",
    "\n",
    "df = df.sort_values('combinedlength', ascending=False)\n",
    "df = df[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai.embeddings_utils import get_embedding\n",
    "# Ensure you have your API key set in your environment per the README: https://github.com/openai/openai-python#usage\n",
    "\n",
    "# This will take just between 5 and 10 minutes\n",
    "#df['ada_similarity'] = df.combined.apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
    "#df['ada_search'] = df.combined.apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ada_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to df2 called embeddings with nothing in it\n",
    "\n",
    "df2['embeddings'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.embeddings[15] = get_embedding(df2.content[15], engine=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('fine_food_reviews_with_embeddings_1k_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv into df\n",
    "\n",
    "df = pd.read_csv('fine_food_reviews_with_embeddings_1k_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ada_search\"] = df.ada_search.apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding = get_embedding(\n",
    "        \"what is product\",\n",
    "        engine=\"text-embedding-ada-002\"\n",
    "    )\n",
    "similarityvector = np.dot(np.array(embedding), np.array(df2[\"embeddings\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"similarities2\"] = df2.embeddings.apply(lambda x: np.dot(np.array(embedding), np.array(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.sort_values(\"similarities2\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETIONS_MODEL = \"text-davinci-002\"\n",
    "\n",
    "prompt = \"\"\"Answer the question as truthfully as possible, and if you're unsure of the answer, say \"Sorry, I don't know\".\n",
    "\n",
    "Q: Who won the 2020 Summer Olympics men's high jump?\n",
    "A:\"\"\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset row index of df2\n",
    "\n",
    "df2 = df2.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the prompt the content from the first few rows of df2 and send request to openai\n",
    "\n",
    "prompt = df2.content[0] + df2.content[1] + df2.content[2] + df2.content[3]\n",
    "\n",
    "prompt2 = \"\"\"  Answer the question as truthfully and in as much detail as possible.\n",
    "\n",
    "Q: what are some common questions asked in a tech product?\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = prompt + prompt2\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "# search through the reviews for a specific product\n",
    "def search_reviews(df, product_description, n=3, pprint=True):\n",
    "    embedding = get_embedding(\n",
    "        product_description,\n",
    "        engine=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    df[\"similarities\"] = df.ada_search.apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "    res = (\n",
    "        df.sort_values(\"similarities\", ascending=False)\n",
    "        .head(n)\n",
    "        .combined.str.replace(\"Title: \", \"\")\n",
    "        .str.replace(\"; Content:\", \": \")\n",
    "    )\n",
    "    if pprint:\n",
    "        for r in res:\n",
    "            print(r[:200])\n",
    "            print()\n",
    "    return res\n",
    "\n",
    "\n",
    "res = search_reviews(df, \"which coffee is best?\", n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai.embeddings_utils import get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff = \"delicious beans delicious beansdelicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans delicious beans \"\n",
    "stuff2 = stuff*10\n",
    "get_embedding(stuff2, engine=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stuff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install feedparser\n",
    "import feedparser\n",
    "\n",
    "def get_articles_from_rss_feed(rss_feed_url):\n",
    "    feed = feedparser.parse(rss_feed_url)\n",
    "    articles = []\n",
    "    for entry in feed.entries:\n",
    "        article = {}\n",
    "        article['title'] = entry.title\n",
    "        article['link'] = entry.link\n",
    "        article['summary'] = entry.summary\n",
    "        articles.append(article)\n",
    "    return articles\n",
    "\n",
    "rss_feed_url = 'https://www.producttalk.org/feed/'\n",
    "articles = get_articles_from_rss_feed(rss_feed_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_feed_url2= 'https://www.producttalk.org/2020/07/opportunity-mapping/'\n",
    "ost = get_articles_from_rss_feed(rss_feed_url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the link of the 5th article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_management_rss =   'https://blog.feedspot.com/product_management_rss_feeds/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parses a url using requests and beautifulsoup and returns links\n",
    "\n",
    "def get_links(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    links = soup.find_all('a', class_='ext')\n",
    "    return links\n",
    "\n",
    "links = get_links(product_management_rss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "\n",
    "for i in links:\n",
    "    urls.append(i['href'])\n",
    "\n",
    "urls = list(filter(None, urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls\n",
    "\n",
    "# remove empty strings from list\n",
    "urls = list(filter(None, urls))\n",
    "\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_articles_from_all_rss_feeds():\n",
    "    links = get_links(product_management_rss)\n",
    "    articles = []\n",
    "    for link in urls:\n",
    "        articles.extend(get_articles_from_rss_feed(link))\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff = get_articles_from_all_rss_feeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = get_links(product_management_rss)\n",
    "articles = []\n",
    "for link in urls:\n",
    "    articles.extend(get_articles_from_rss_feed(link))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#scrape text from each article link in articles\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'}\n",
    "\n",
    "user_agents_list = [\n",
    "    'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'\n",
    "]\n",
    "\n",
    "def scrape_text_from_article(article):\n",
    "    r = requests.get(article['link'], headers={'User-Agent': random.choice(user_agents_list)})\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    article['content'] = soup.get_text()\n",
    "    return article\n",
    "\n",
    "scrape_text_from_article(articles[0])\n",
    "\n",
    "#scrape text from all articles\n",
    "\n",
    "def scrape_text_from_all_articles(articles):\n",
    "    for article in articles:\n",
    "        scrape_text_from_article(article)\n",
    "    return articles\n",
    "\n",
    "scrape_text_from_all_articles(articles[1050:])\n",
    "\n",
    "#remove articles that don't have content\n",
    "\n",
    "def remove_articles_without_content(articles):\n",
    "    return [article for article in articles if 'content' in article]\n",
    "\n",
    "articles = remove_articles_without_content(articles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check which articles have content\n",
    "\n",
    "articles[1050]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'}\n",
    "r = requests.get('https://www.producttalk.org/2022/12/customer-interviews/?utm_source=rss&utm_medium=rss&utm_campaign=customer-interviews#whats-a-customer-interview', headers=HEADERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dfarticles = pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfarticles.to_csv('rss_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfarticles = pd.read_csv('rss_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfarticles[\"contentlength\"] = dfarticles[\"content\"].str.len()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfarticles.contentlength.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfarticles[\"embeddings\"] = dfarticles.content.apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to get embedding for each article using a for statement and catch retry errors\n",
    "embeddings = []\n",
    "\n",
    "for i in dfarticles.content:\n",
    "    try:\n",
    "        embeddings.append(get_embedding(i, engine='text-embedding-ada-002'))\n",
    "    except:\n",
    "        embeddings.append('error')\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfarticles[\"embeddings\"] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfarticles.to_csv('rss_articles_embeddings.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfarticles = pd.read_csv('rss_articles_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip all new lines from the content of each article\n",
    "\n",
    "dfarticles[\"content\"] = dfarticles.content.str.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfarticles.content[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#break up the content of each article into 1000 word chunks\n",
    "\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "dfarticles[\"contentchunks\"] = dfarticles.content.apply(lambda x: [nltk.word_tokenize(x)[i:i+1000] for i in range(0, len(nltk.word_tokenize(x)), 1000)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfarticles.contentchunks[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#something = dfarticles[\"contentchunks\"].apply(lambda x: [' '.join(chunk) for chunk in x])\n",
    "\n",
    "#something = dfarticles[\"contentchunks\"].apply(lambda x: [[' '.join(sub_chunk) for sub_chunk in chunk] for chunk in x])\n",
    "\n",
    "something = dfarticles[\"contentchunks\"].apply(lambda x: [' '.join(sub_chunk) for chunk in x for sub_chunk in chunk])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_list_of_words = dfarticles.contentchunks[0][0]\n",
    "combined_string = ' '.join(first_list_of_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedstrings = []\n",
    "for i in dfarticles.contentchunks:\n",
    "    for lst in i:\n",
    "        combined_string = ' '.join([word for word in lst])\n",
    "        combinedstrings.append(combined_string)\n",
    "        print(len(combined_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfarticles.contentchunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedstringsouter = []\n",
    "\n",
    "\n",
    "for i in dfarticles.contentchunks:\n",
    "    combinedstringsinner = []\n",
    "    \n",
    "    for lst in i:\n",
    "        combined_string = ' '.join([word for word in lst])\n",
    "        combinedstringsinner.append(combined_string)\n",
    "        print(len(combined_string))\n",
    "\n",
    "    combinedstringsouter.append(combinedstringsinner)\n",
    "\n",
    "    \n",
    "    #add combined strings to a new column in dfarticles to the corresponding row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfarticles[\"combinedcontentchunks\"] = combinedstringsouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop combined strings from dfarticles\n",
    "\n",
    "dfarticles = dfarticles.drop(columns=['combinedstrings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfarticles.to_csv('rss_articles_embeddings_chunked.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfarticles.combinedcontentchunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new dataframe with each chunk of content as a row and keep the appropriate article metadata\n",
    "\n",
    "dfchunks = dfarticles.explode('combinedcontentchunks')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfchunks.combinedcontentchunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embedding for each chunk of content\n",
    "\n",
    "dfchunks[\"chunkembeddings\"] = dfchunks.combinedcontentchunks.apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex the dataframe\n",
    "\n",
    "dfchunks = dfchunks.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>summary</th>\n",
       "      <th>content</th>\n",
       "      <th>contentlength</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>contentchunks</th>\n",
       "      <th>combinedcontentchunks</th>\n",
       "      <th>chunkembeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Slowing Down: Choosing Not to Set Goals for 2023</td>\n",
       "      <td>https://www.producttalk.org/2023/01/slowing-do...</td>\n",
       "      <td>&lt;p&gt;Happy New Year! For the past several years,...</td>\n",
       "      <td>Slowing Down: Choosing Not to Set Goals...</td>\n",
       "      <td>15556</td>\n",
       "      <td>[-0.025955602526664734, -0.014085495844483376,...</td>\n",
       "      <td>[[Slowing, Down, :, Choosing, Not, to, Set, Go...</td>\n",
       "      <td>Slowing Down : Choosing Not to Set Goals for 2...</td>\n",
       "      <td>[-0.02125050686299801, -0.02566228248178959, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Slowing Down: Choosing Not to Set Goals for 2023</td>\n",
       "      <td>https://www.producttalk.org/2023/01/slowing-do...</td>\n",
       "      <td>&lt;p&gt;Happy New Year! For the past several years,...</td>\n",
       "      <td>Slowing Down: Choosing Not to Set Goals...</td>\n",
       "      <td>15556</td>\n",
       "      <td>[-0.025955602526664734, -0.014085495844483376,...</td>\n",
       "      <td>[[Slowing, Down, :, Choosing, Not, to, Set, Go...</td>\n",
       "      <td>I now had three new courses to iterate on and ...</td>\n",
       "      <td>[-0.01898626983165741, -0.03097466379404068, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Slowing Down: Choosing Not to Set Goals for 2023</td>\n",
       "      <td>https://www.producttalk.org/2023/01/slowing-do...</td>\n",
       "      <td>&lt;p&gt;Happy New Year! For the past several years,...</td>\n",
       "      <td>Slowing Down: Choosing Not to Set Goals...</td>\n",
       "      <td>15556</td>\n",
       "      <td>[-0.025955602526664734, -0.014085495844483376,...</td>\n",
       "      <td>[[Slowing, Down, :, Choosing, Not, to, Set, Go...</td>\n",
       "      <td>Kittler â€™ s new conference , Product at Heart ...</td>\n",
       "      <td>[-0.016916798427700996, -0.008791188709437847,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Slowing Down: Choosing Not to Set Goals for 2023</td>\n",
       "      <td>https://www.producttalk.org/2023/01/slowing-do...</td>\n",
       "      <td>&lt;p&gt;Happy New Year! For the past several years,...</td>\n",
       "      <td>Slowing Down: Choosing Not to Set Goals...</td>\n",
       "      <td>15556</td>\n",
       "      <td>[-0.025955602526664734, -0.014085495844483376,...</td>\n",
       "      <td>[[Slowing, Down, :, Choosing, Not, to, Set, Go...</td>\n",
       "      <td>Product Talk Academy and is the author of Cont...</td>\n",
       "      <td>[-0.01252877525985241, -0.006119410041719675, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Defining Product Outcomes: The 8 Most Common M...</td>\n",
       "      <td>https://www.producttalk.org/2022/12/defining-p...</td>\n",
       "      <td>&lt;p&gt;I recently sat down with fellow Product Tal...</td>\n",
       "      <td>Defining Product Outcomes: The 8 Most C...</td>\n",
       "      <td>17380</td>\n",
       "      <td>[-0.021616747602820396, 0.009200184606015682, ...</td>\n",
       "      <td>[[Defining, Product, Outcomes, :, The, 8, Most...</td>\n",
       "      <td>Defining Product Outcomes : The 8 Most Common ...</td>\n",
       "      <td>[-0.017809269949793816, 0.009950662031769753, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3390</th>\n",
       "      <td>1434</td>\n",
       "      <td>The Decade of Gen X Wish Fulfillment</td>\n",
       "      <td>https://adamnash.blog/2017/01/14/the-decade-of...</td>\n",
       "      <td>At 9:54am this morning in California, a Falcon...</td>\n",
       "      <td>The Decade of Gen X Wish Fulfillment | Ps...</td>\n",
       "      <td>10179</td>\n",
       "      <td>[-0.0034262132830917835, -0.02629011683166027,...</td>\n",
       "      <td>[[The, Decade, of, Gen, X, Wish, Fulfillment, ...</td>\n",
       "      <td>The Decade of Gen X Wish Fulfillment | Psychoh...</td>\n",
       "      <td>[0.004289509728550911, -0.027995409443974495, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3391</th>\n",
       "      <td>1434</td>\n",
       "      <td>The Decade of Gen X Wish Fulfillment</td>\n",
       "      <td>https://adamnash.blog/2017/01/14/the-decade-of...</td>\n",
       "      <td>At 9:54am this morning in California, a Falcon...</td>\n",
       "      <td>The Decade of Gen X Wish Fulfillment | Ps...</td>\n",
       "      <td>10179</td>\n",
       "      <td>[-0.0034262132830917835, -0.02629011683166027,...</td>\n",
       "      <td>[[The, Decade, of, Gen, X, Wish, Fulfillment, ...</td>\n",
       "      <td>giants . The previous generation gave us the e...</td>\n",
       "      <td>[-0.012086725793778896, -0.023464512079954147,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3392</th>\n",
       "      <td>1435</td>\n",
       "      <td>2017: New Year, New Template, More Posts</td>\n",
       "      <td>https://adamnash.blog/2017/01/03/2017-new-year...</td>\n",
       "      <td>2017 is finally here. Over the past few years,...</td>\n",
       "      <td>2017: New Year, New Template, More Posts ...</td>\n",
       "      <td>5012</td>\n",
       "      <td>[-0.01477749738842249, 0.009537720121443272, 0...</td>\n",
       "      <td>[[2017, :, New, Year, ,, New, Template, ,, Mor...</td>\n",
       "      <td>2017 : New Year , New Template , More Posts | ...</td>\n",
       "      <td>[-0.01684506982564926, 0.0076107243075966835, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3393</th>\n",
       "      <td>1436</td>\n",
       "      <td>Not Everyone Has a Grandma Flora</td>\n",
       "      <td>https://adamnash.blog/2016/01/13/not-everyone-...</td>\n",
       "      <td>My grandmother, Flora, might be the reason I e...</td>\n",
       "      <td>Not Everyone Has a Grandma Flora | Psycho...</td>\n",
       "      <td>5664</td>\n",
       "      <td>[0.004130902234464884, -0.008354115299880505, ...</td>\n",
       "      <td>[[Not, Everyone, Has, a, Grandma, Flora, |, Ps...</td>\n",
       "      <td>Not Everyone Has a Grandma Flora | Psychohisto...</td>\n",
       "      <td>[0.0071901739574968815, -0.009906611405313015,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3394</th>\n",
       "      <td>1436</td>\n",
       "      <td>Not Everyone Has a Grandma Flora</td>\n",
       "      <td>https://adamnash.blog/2016/01/13/not-everyone-...</td>\n",
       "      <td>My grandmother, Flora, might be the reason I e...</td>\n",
       "      <td>Not Everyone Has a Grandma Flora | Psycho...</td>\n",
       "      <td>5664</td>\n",
       "      <td>[0.004130902234464884, -0.008354115299880505, ...</td>\n",
       "      <td>[[Not, Everyone, Has, a, Grandma, Flora, |, Ps...</td>\n",
       "      <td>subscriptions Collapse this bar Loading Commen...</td>\n",
       "      <td>[-0.01988794095814228, -0.003987910691648722, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3395 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              title  \\\n",
       "0              0   Slowing Down: Choosing Not to Set Goals for 2023   \n",
       "1              0   Slowing Down: Choosing Not to Set Goals for 2023   \n",
       "2              0   Slowing Down: Choosing Not to Set Goals for 2023   \n",
       "3              0   Slowing Down: Choosing Not to Set Goals for 2023   \n",
       "4              1  Defining Product Outcomes: The 8 Most Common M...   \n",
       "...          ...                                                ...   \n",
       "3390        1434               The Decade of Gen X Wish Fulfillment   \n",
       "3391        1434               The Decade of Gen X Wish Fulfillment   \n",
       "3392        1435           2017: New Year, New Template, More Posts   \n",
       "3393        1436                   Not Everyone Has a Grandma Flora   \n",
       "3394        1436                   Not Everyone Has a Grandma Flora   \n",
       "\n",
       "                                                   link  \\\n",
       "0     https://www.producttalk.org/2023/01/slowing-do...   \n",
       "1     https://www.producttalk.org/2023/01/slowing-do...   \n",
       "2     https://www.producttalk.org/2023/01/slowing-do...   \n",
       "3     https://www.producttalk.org/2023/01/slowing-do...   \n",
       "4     https://www.producttalk.org/2022/12/defining-p...   \n",
       "...                                                 ...   \n",
       "3390  https://adamnash.blog/2017/01/14/the-decade-of...   \n",
       "3391  https://adamnash.blog/2017/01/14/the-decade-of...   \n",
       "3392  https://adamnash.blog/2017/01/03/2017-new-year...   \n",
       "3393  https://adamnash.blog/2016/01/13/not-everyone-...   \n",
       "3394  https://adamnash.blog/2016/01/13/not-everyone-...   \n",
       "\n",
       "                                                summary  \\\n",
       "0     <p>Happy New Year! For the past several years,...   \n",
       "1     <p>Happy New Year! For the past several years,...   \n",
       "2     <p>Happy New Year! For the past several years,...   \n",
       "3     <p>Happy New Year! For the past several years,...   \n",
       "4     <p>I recently sat down with fellow Product Tal...   \n",
       "...                                                 ...   \n",
       "3390  At 9:54am this morning in California, a Falcon...   \n",
       "3391  At 9:54am this morning in California, a Falcon...   \n",
       "3392  2017 is finally here. Over the past few years,...   \n",
       "3393  My grandmother, Flora, might be the reason I e...   \n",
       "3394  My grandmother, Flora, might be the reason I e...   \n",
       "\n",
       "                                                content  contentlength  \\\n",
       "0            Slowing Down: Choosing Not to Set Goals...          15556   \n",
       "1            Slowing Down: Choosing Not to Set Goals...          15556   \n",
       "2            Slowing Down: Choosing Not to Set Goals...          15556   \n",
       "3            Slowing Down: Choosing Not to Set Goals...          15556   \n",
       "4            Defining Product Outcomes: The 8 Most C...          17380   \n",
       "...                                                 ...            ...   \n",
       "3390       The Decade of Gen X Wish Fulfillment | Ps...          10179   \n",
       "3391       The Decade of Gen X Wish Fulfillment | Ps...          10179   \n",
       "3392       2017: New Year, New Template, More Posts ...           5012   \n",
       "3393       Not Everyone Has a Grandma Flora | Psycho...           5664   \n",
       "3394       Not Everyone Has a Grandma Flora | Psycho...           5664   \n",
       "\n",
       "                                             embeddings  \\\n",
       "0     [-0.025955602526664734, -0.014085495844483376,...   \n",
       "1     [-0.025955602526664734, -0.014085495844483376,...   \n",
       "2     [-0.025955602526664734, -0.014085495844483376,...   \n",
       "3     [-0.025955602526664734, -0.014085495844483376,...   \n",
       "4     [-0.021616747602820396, 0.009200184606015682, ...   \n",
       "...                                                 ...   \n",
       "3390  [-0.0034262132830917835, -0.02629011683166027,...   \n",
       "3391  [-0.0034262132830917835, -0.02629011683166027,...   \n",
       "3392  [-0.01477749738842249, 0.009537720121443272, 0...   \n",
       "3393  [0.004130902234464884, -0.008354115299880505, ...   \n",
       "3394  [0.004130902234464884, -0.008354115299880505, ...   \n",
       "\n",
       "                                          contentchunks  \\\n",
       "0     [[Slowing, Down, :, Choosing, Not, to, Set, Go...   \n",
       "1     [[Slowing, Down, :, Choosing, Not, to, Set, Go...   \n",
       "2     [[Slowing, Down, :, Choosing, Not, to, Set, Go...   \n",
       "3     [[Slowing, Down, :, Choosing, Not, to, Set, Go...   \n",
       "4     [[Defining, Product, Outcomes, :, The, 8, Most...   \n",
       "...                                                 ...   \n",
       "3390  [[The, Decade, of, Gen, X, Wish, Fulfillment, ...   \n",
       "3391  [[The, Decade, of, Gen, X, Wish, Fulfillment, ...   \n",
       "3392  [[2017, :, New, Year, ,, New, Template, ,, Mor...   \n",
       "3393  [[Not, Everyone, Has, a, Grandma, Flora, |, Ps...   \n",
       "3394  [[Not, Everyone, Has, a, Grandma, Flora, |, Ps...   \n",
       "\n",
       "                                  combinedcontentchunks  \\\n",
       "0     Slowing Down : Choosing Not to Set Goals for 2...   \n",
       "1     I now had three new courses to iterate on and ...   \n",
       "2     Kittler â€™ s new conference , Product at Heart ...   \n",
       "3     Product Talk Academy and is the author of Cont...   \n",
       "4     Defining Product Outcomes : The 8 Most Common ...   \n",
       "...                                                 ...   \n",
       "3390  The Decade of Gen X Wish Fulfillment | Psychoh...   \n",
       "3391  giants . The previous generation gave us the e...   \n",
       "3392  2017 : New Year , New Template , More Posts | ...   \n",
       "3393  Not Everyone Has a Grandma Flora | Psychohisto...   \n",
       "3394  subscriptions Collapse this bar Loading Commen...   \n",
       "\n",
       "                                        chunkembeddings  \n",
       "0     [-0.02125050686299801, -0.02566228248178959, 0...  \n",
       "1     [-0.01898626983165741, -0.03097466379404068, 0...  \n",
       "2     [-0.016916798427700996, -0.008791188709437847,...  \n",
       "3     [-0.01252877525985241, -0.006119410041719675, ...  \n",
       "4     [-0.017809269949793816, 0.009950662031769753, ...  \n",
       "...                                                 ...  \n",
       "3390  [0.004289509728550911, -0.027995409443974495, ...  \n",
       "3391  [-0.012086725793778896, -0.023464512079954147,...  \n",
       "3392  [-0.01684506982564926, 0.0076107243075966835, ...  \n",
       "3393  [0.0071901739574968815, -0.009906611405313015,...  \n",
       "3394  [-0.01988794095814228, -0.003987910691648722, ...  \n",
       "\n",
       "[3395 rows x 10 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfchunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe to a csv\n",
    "\n",
    "dfchunks.to_csv('rss_articles_embeddings_chunked_embedded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfchunks.chunkembeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the similarity between the embedding of a chunk of content and the embedding of a question\n",
    "import numpy as np\n",
    "\n",
    "embedding = get_embedding(\n",
    "        \"what is product\",\n",
    "        engine=\"text-embedding-ada-002\"\n",
    "    )\n",
    "similarityvector = np.dot(np.array(embedding), np.array(dfchunks[\"chunkembeddings\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the question and get embedding for the question\n",
    "\n",
    "question = \"what problems currently exist in product management for a new product manager and how should one address each of them, please share a plan. \\n\"\n",
    "\n",
    "embeddingpm = get_embedding(\n",
    "        question,\n",
    "        engine=\"text-embedding-ada-002\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the similarity between the embedding of a chunk of content and the embedding of a question\n",
    "\n",
    "dfchunks[\"similarities\"] = dfchunks.chunkembeddings.apply(lambda x: np.dot(np.array(embeddingpm), np.array(x)))\n",
    "df = dfchunks.sort_values(\"similarities\", ascending=False).head(3)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As a new product manager, there are several common challenges that you may face. These include: \\n\\n1. Poor definition of role: As a new product manager, it is important to have a clear understanding of your role and responsibilities. This includes understanding the scope of your role, the stakeholders you will be working with, and the goals and objectives of the product. It is important to have a clear understanding of the product vision and strategy, and how your role fits into the overall product development process.\\n\\n2. Having too much to do: As a new product manager, it is easy to become overwhelmed with the amount of work that needs to be done. It is important to prioritize tasks and focus on the most important ones first. It is also important to delegate tasks to other team members when possible.\\n\\n3. Lack of ownership of the roadmap: As a new product manager, it is important to take ownership of the product roadmap. This includes understanding the product vision and strategy, and how the roadmap will help to achieve the desired goals. It is also important to ensure that the roadmap is aligned with the product vision and strategy, and that it is regularly updated to reflect changes in the market and customer needs.\\n\\n4. Dealing with vague business strategies: As a new product manager, it is important to have a clear understanding of the business strategy and how it relates to the product. This includes understanding the target market, the competitive landscape, and the customer needs. It is also important to ensure that the product strategy is aligned with the business strategy, and that it is regularly updated to reflect changes in the market and customer needs.\\n\\n5. Missing a common way of working and language: As a new product manager, it is important to have a common language and way of working with the team. This includes understanding the product development process, the tools and techniques used, and the roles and responsibilities of each team member. It is also important to ensure that everyone is on the same page and that there is a clear understanding of the product vision and strategy.\\n\\nTo address these challenges, I would recommend the following plan:\\n\\n1. Develop a clear understanding of your role and responsibilities: Take the time to understand the scope of your role, the stakeholders you will be working with, and the goals and objectives of the product.\\n\\n2. Prioritize tasks and delegate when possible: Prioritize tasks and focus on the most important ones first. Delegate tasks to other team members when possible.\\n\\n3. Take ownership of the product roadmap: Understand the product vision and strategy, and how the roadmap will help to achieve the desired goals. Ensure that the roadmap is aligned with the product vision and strategy, and that it is regularly updated to reflect changes in the market and customer needs.\\n\\n4. Understand the business strategy and how it relates to the product: Understand the target market, the competitive landscape, and the customer needs.'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create prompt and send request to da vinci api\n",
    "\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "\n",
    "prompt = df.combinedcontentchunks[0] + df.combinedcontentchunks[1] + df.combinedcontentchunks[2]\n",
    "\n",
    "prompt2 = \"\"\"\\n  You are an experienced software product management expert. Answer the question based on the context above as truthfully and in as much detail as possible.\\n\n",
    "Q: \"\"\" + question + \"\"\"\\n\n",
    "A: \"\"\"\n",
    "\n",
    "prompt = prompt + prompt2\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=600,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through 500 articles and make a call to the openai api for each article to create question answer pairs\n",
    "\n",
    "#save first 500 rows of dfchunks in dffinetuned\n",
    "dffinetuned = dfchunks[0:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>summary</th>\n",
       "      <th>content</th>\n",
       "      <th>contentlength</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>contentchunks</th>\n",
       "      <th>combinedcontentchunks</th>\n",
       "      <th>chunkembeddings</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Slowing Down: Choosing Not to Set Goals for 2023</td>\n",
       "      <td>https://www.producttalk.org/2023/01/slowing-do...</td>\n",
       "      <td>&lt;p&gt;Happy New Year! For the past several years,...</td>\n",
       "      <td>Slowing Down: Choosing Not to Set Goals...</td>\n",
       "      <td>15556</td>\n",
       "      <td>[-0.025955602526664734, -0.014085495844483376,...</td>\n",
       "      <td>[[Slowing, Down, :, Choosing, Not, to, Set, Go...</td>\n",
       "      <td>Slowing Down : Choosing Not to Set Goals for 2...</td>\n",
       "      <td>[-0.02125050686299801, -0.02566228248178959, 0...</td>\n",
       "      <td>0.788939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             title  \\\n",
       "0           0  Slowing Down: Choosing Not to Set Goals for 2023   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.producttalk.org/2023/01/slowing-do...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  <p>Happy New Year! For the past several years,...   \n",
       "\n",
       "                                             content  contentlength  \\\n",
       "0         Slowing Down: Choosing Not to Set Goals...          15556   \n",
       "\n",
       "                                          embeddings  \\\n",
       "0  [-0.025955602526664734, -0.014085495844483376,...   \n",
       "\n",
       "                                       contentchunks  \\\n",
       "0  [[Slowing, Down, :, Choosing, Not, to, Set, Go...   \n",
       "\n",
       "                               combinedcontentchunks  \\\n",
       "0  Slowing Down : Choosing Not to Set Goals for 2...   \n",
       "\n",
       "                                     chunkembeddings  similarities  \n",
       "0  [-0.02125050686299801, -0.02566228248178959, 0...      0.788939  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffinetuned.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make call to openai completion api for content in each article in dfarticles\n",
    "\n",
    "promptaddition = \"\\nThe above is an excerpt from an article on software product management. Can you please rewrite the above article in question answer pairs below? Don't lose any detail in the answer and explain the answer portion extensively. Add enough detail (like the who, what, when, where, why, how) to the questions so they can be understood without context:\\n\"\n",
    "\n",
    "def get_completion(text):\n",
    "    response = openai.Completion.create(\n",
    "    prompt=text + promptaddition,\n",
    "    temperature=0,\n",
    "    max_tokens=800,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a call to the openai api for each article to create question answer pairs\n",
    "\n",
    "test = get_completion(dffinetuned.combinedcontentchunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q: What is the title of Teresa Torres\\' new book?\\nA: Teresa Torres\\' new book is titled \"Continuous Discovery Habits\". It is a guide to a structured and sustainable approach to continuous discovery. \\n\\nQ: What was the decision Teresa Torres made in May 2020?\\nA: In May 2020, Teresa Torres made the decision to develop products that individuals could buy. She believed that while companies might pull back on spending during recessions, individuals tend to invest in themselves. \\n\\nQ: What was the impact of switching to a course-first business?\\nA: Switching to a course-first business had a major impact on the number of people Teresa Torres and her team could reach. It allowed them to scale their impact by working with more teams at more companies. It also provided a more cost-effective way for teams to learn the basics of their continuous discovery framework. \\n\\nQ: What did Teresa Torres do in December 2019?\\nA: In December 2019, Teresa Torres published a state of the business report as her initial blog post of the year. She reflected on the past year and looked at the year ahead.'"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " \"What is the title of Teresa Torres' new book?\\n\",\n",
       " 'Teresa Torres\\' new book is titled \"Continuous Discovery Habits\". It is a guide to a structured and sustainable approach to continuous discovery. \\n\\n',\n",
       " 'What was the decision Teresa Torres made in May 2020?\\n',\n",
       " 'In May 2020, Teresa Torres made the decision to develop products that individuals could buy. She believed that while companies might pull back on spending during recessions, individuals tend to invest in themselves. \\n\\n',\n",
       " 'What was the impact of switching to a course-first business?\\n',\n",
       " 'Switching to a course-first business had a major impact on the number of people Teresa Torres and her team could reach. It allowed them to scale their impact by working with more teams at more companies. It also provided a more cost-effective way for teams to learn the basics of their continuous discovery framework. \\n\\n',\n",
       " 'What did Teresa Torres do in December 2019?\\n',\n",
       " 'In December 2019, Teresa Torres published a state of the business report as her initial blog post of the year. She reflected on the past year and looked at the year ahead.']"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the string in test and split it into question answer pairs where questions are preceded by \"Q: \" and answers are preceded by \"A: \"\n",
    "\n",
    "import re\n",
    "\n",
    "def get_qa_pairs(text):\n",
    "    qa_pairs = re.split(r'Q: |A: ', text)\n",
    "    return qa_pairs\n",
    "\n",
    "get_qa_pairs(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annas\\AppData\\Local\\Temp\\ipykernel_2968\\954458313.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dffinetuned[\"questionanswerpairs\"] = dffinetuned.combinedcontentchunks.apply(lambda x: get_completion(x))\n"
     ]
    }
   ],
   "source": [
    "# make a call to the openai api for each article to create question answer pairs\n",
    "\n",
    "dffinetuned[\"questionanswerpairs\"] = dffinetuned.combinedcontentchunks.apply(lambda x: get_completion(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffinetuned.to_csv('dffinetuned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annas\\AppData\\Local\\Temp\\ipykernel_2968\\2916898042.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dffinetuned[\"qapairsformatted\"] = dffinetuned.questionanswerpairs.apply(lambda x: get_qa_pairs(x))\n"
     ]
    }
   ],
   "source": [
    "dffinetuned[\"qapairsformatted\"] = dffinetuned.questionanswerpairs.apply(lambda x: get_qa_pairs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annas\\AppData\\Local\\Temp\\ipykernel_2968\\2234546820.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dffinetuned[\"qapairsformatted\"] = dffinetuned.qapairsformatted.apply(lambda x: x[1:])\n"
     ]
    }
   ],
   "source": [
    "# drop the first item in each list in qapairsformatted because it is an empty string\n",
    "\n",
    "dffinetuned[\"qapairsformatted\"] = dffinetuned.qapairsformatted.apply(lambda x: x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "explodeddffinetuned = dffinetuned.explode('qapairsformatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explodeddffinetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuningdata = explodeddffinetuned[\"qapairsformatted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annas\\AppData\\Local\\Temp\\ipykernel_2968\\1189859343.py:3: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  questions = finetuningdata[0::2]\n",
      "C:\\Users\\annas\\AppData\\Local\\Temp\\ipykernel_2968\\1189859343.py:4: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  answers = finetuningdata[1::2]\n"
     ]
    }
   ],
   "source": [
    "# take all odd rows in finetuningdata and make them questions and all even rows in finetuningdata and make them answers\n",
    "\n",
    "questions = finetuningdata[0::2]\n",
    "answers = finetuningdata[1::2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      What is the title of Teresa Torres' new book?\\n\n",
       "0    What was the decision Teresa Torres made in Ma...\n",
       "0    What was the impact of switching to a course-f...\n",
       "Name: qapairsformatted, dtype: object"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1efa8da88f1a547ece805b348be336bfd12fbe90d460a14aa759d01b5dfa4db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
